project:
  name: "xiaobeir1"
  description: "under-construction"

experiment:
  name: "qwen2.5-32b-medqa-4GPU"  # 实验名称，对应 experiments/ 下的目录
  random_seed: 42

model:
  name: "Qwen/Qwen2.5-0.5B-Instruct"
  # name: "Qwen/Qwen2.5-7B-Instruct"
  # name: "Qwen/Qwen2.5-14B-Instruct"
  # name: "Qwen/Qwen2.5-32B-Instruct"
  torch_dtype: "bfloat16"
  device_map: null

dataset:
  name: "medqa"
  # name: "medmcqa"
  num_eval: 1000

training:
  continue_training: false
  current_step: 0

  use_lora: true
  use_quant: true
  batch_size: 1
  learning_rate: 0.000005
  num_iterations: 2 # epoch
  steps_per_iteration: 5 # in one epoch
  save_interval: 5 # steps

  generation:
    num_generations: 4
    max_new_tokens: 200
    max_length_for_gather: 100000
    max_generate_iterations: 3
    temperature: 0.7
    do_sample: True
  
  optimizer:
    beta: 0.04
    mu: 1
    epsilon: 0.1

lora:
  r: 8
  lora_alpha: 32
  target_modules:
    - "q_proj"    # qwen
    - "v_proj"    # qwen
  lora_dropout: 0.1
  bias: "none"
  task_type: "CAUSAL_LM"

qlora:
  load_in_4bit: False           # zero 2 可以为True； zero 3必须为False
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_compute_dtype: "bfloat16"
  bnb_4bit_use_double_quant: False   # zero 2 可以为True； zero 3必须为False
  load_in_8bit: True    # enable 8bit quantization  ； zero 3可以为True
  llm_int8_threshold: 8.0   # if load_in_8bit is True ； zero 3可以为True


  ### This is the suggested for zero 2 -- reference：https://huggingface.co/docs/accelerate/v1.6.0/en/package_reference/utilities#accelerate.utils.BnbQuantizationConfig
  # qlora:
  # load_in_4bit: True           # zero 2 可以为True； zero 3必须为False
  # bnb_4bit_quant_type: "nf4"
  # bnb_4bit_compute_dtype: "bfloat16"
  # bnb_4bit_use_double_quant: True   # zero 2 可以为True； zero 3必须为False
  # load_in_8bit: False    # enable 8bit quantization  ； zero 3可以为True
  # llm_int8_threshold: 6.0   # if load_in_8bit is True ； zero 3可以为True