# ğŸ¤– Agentic RAG-R1: Enhance Agentic RAG Reasoning Capacity via Reinforcement Learning ğŸš€

## Table of Contents
- [Introduction](#introduction)
  - [What is Agentic RAG?](#what-is-agentic-rag)
  - [Architecture](#architecture)
  - [Training Strategy](#training-strategy)
  - [Rollout Generation](#rollout-generation)
- [Installation](#installation)
  - [Tools Environment](#tools-environment-optional)
  - [Folder Structure](#folder-structure)
  - [Quick Start](#quick-start)
- [Features](#features)
- [Results](#results)
- [Roadmap](#roadmap)
- [Acknowledgements](#acknowledgements)
- [Citation](#citation)
- [License](#license)

## Introduction ğŸŒŸ

Agentic RAGâ€‘R1 is an openâ€‘source initiative to build an Agentic Retrievalâ€‘Augmented Generation (RAG) system by endowing a base language model with autonomous search & reasoning skills through reinforcement learning (currently using the GRPO algorithm). 

**Chinese Language Version:**

![Chinese version results](https://github.com/user-attachments/assets/a6e42d35-4fec-43b9-9a04-3d102e544e20)


**English Language Version:**


![English version results](https://github.com/user-attachments/assets/40f11648-bf46-4cd3-873c-78ca63069499)


### What is Agentic RAG? ğŸ’¡

Agentic RAG combines two powerful concepts:

- **Retrievalâ€‘Augmented Generation (RAG)**: Combines generative power with onâ€‘theâ€‘fly retrieval from external knowledge bases, ensuring factual and upâ€‘toâ€‘date answers.
- **Agentic AI**: Gives the model the ability to decide when to retrieve, what to retrieve, and how to weave the retrieved evidence into its reasoning.

![Agentic RAG concept](https://github.com/user-attachments/assets/7b4b6559-b395-4de0-8326-ad0fca2e671a)

### Architecture ğŸ—ï¸

Our architecture is inspired by TCâ€‘RAG and features an agent memory stack that orchestrates the full deliberation loop, supporting the following actions:

1. Plan (âŒ)
2. Reasoning (âœ…)
3. Backtrack (âœ…)
4. Summary (âœ…)
5. Tool Observation â€“ wiki/document/knowledgeâ€‘graph search, etc. (âœ…)
6. Conclusion (âœ…)

![Architecture diagram](https://github.com/user-attachments/assets/53dfae56-6c59-488f-9313-7688d5839077)

### Training Strategy ğŸ§ 

Motivated by DeepSeek-R1, we apply GRPO (Generalized Relevance Policy Optimization) to reinforce the agent's choice of reasoning steps and retrieval actions, effectively boosting both search depth and answer quality.

![Training strategy diagram](https://github.com/user-attachments/assets/9880394a-f16a-4acd-84c8-db9f4f7d8433)

### Rollout Generation ğŸ”„

![Rollout generation diagram](https://github.com/user-attachments/assets/21d90097-f7a4-46ef-a442-c8a0a778bab4)

## Installation ğŸ› ï¸

We use conda to manage the environment. Follow these steps to set up:

```bash
conda create -n AgenticRAG python=3.11 -y
conda activate AgenticRAG 
pip install -r requirements.txt
```

### Tools Environment (Optional) ğŸ§°

We provide our search tool repository [ArtSearch](https://github.com/Artessay/ArtSearch) as the search engine, which supports retrieval of information from Wikipedia. You can follow the instructions in that repository to deploy a local instance of the search system.

### Folder Structure ğŸ“

```
.
â”œâ”€â”€ ArtSearch                 # Search tool integration
â”œâ”€â”€ checkpoints               # Model checkpoints
â”œâ”€â”€ examples                  # Example use cases
â”œâ”€â”€ experiments
â”‚   â”œâ”€â”€ evaluation            # Evaluation scripts and results
â”‚   â””â”€â”€ training              # Training configurations
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ script
â”‚   â”œâ”€â”€ evaluation            # Evaluation scripts
â”‚   â”œâ”€â”€ run_server.sh         # Server deployment script
â”‚   â””â”€â”€ training              # Training scripts
â”œâ”€â”€ service
â”‚   â”œâ”€â”€ chat_client.py        # Client for interacting with the model
â”‚   â””â”€â”€ chat_server.py        # Server for hosting the model
â”œâ”€â”€ src
â”‚   â”œâ”€â”€ config                # Configuration files
â”‚   â”œâ”€â”€ data                  # Data processing utilities
â”‚   â”œâ”€â”€ evaluation            # Evaluation metrics and tools
â”‚   â”œâ”€â”€ models                # Model definitions
â”‚   â”œâ”€â”€ train.py              # Main training script
â”‚   â””â”€â”€ utils                 # Utility functions
```

### Quick Start âš¡

#### Training

comming soon~

#### Inference

comming soon~

## Features âœ¨

- **LoRA Tuning Support** ğŸ”§: Fine-tune efficiently with Low-Rank Adaptation
- **Custom Agent Tools** ğŸ› ï¸: Integrate your own tools and personal RAG datasets
- **Distributed Training** ğŸŒ: Support for Deepspeed Zero 2 Stage and Zero 3 Stage
- **Efficient Resource Usage** ğŸ’»: Support for models up to 32B parameters using only 2 A100 GPUs
- **Tool Calling Reward** ğŸ¯: Enhanced reward model that includes:
  - Accuracy reward
  - Format reward
  - RAG accuracy reward using the RAGAS framework

  The total reward is calculated as:

  $$r_{total} = r_{accuracy} + r_{format} + r_{rag}$$

- **TCRAG Integration** ğŸ”—: Uses TCRAG as the rollout generator

## Results ğŸ“Š

### Experiment Log on Qwen 2.5-7B-Instruct

![Experiment log](https://github.com/user-attachments/assets/591d61aa-d4e4-45e9-a48a-77a01858a24b)

We have made our training logs publicly available at: [SwanLab Training Log](https://swanlab.cn/@devilran/xiaobeir1/runs/ipuoxctxo764rvub20d6h/chart)

### Results on MedQA Test Set ğŸ¥

Our Qwen 2.5-7B-Instruct model was evaluated on the MedQA test set using Qwenâ€‘2.5â€‘72B as the judge:

| Configuration                        | Format Accuracy | Answer Accuracy |
|-------------------------------------|------------------|------------------|
| Before fine-tuning                  | 39%              | 84%              |
| Before fine-tuning + search         | 56%              | 79%              |
| After fine-tuning (200 steps) + search | 92%              | 87%              |


## Roadmap ğŸ—ºï¸

- [ ] Support model quantification
- [ ] Add more tools
- [ ] [Additional planned features]

## Acknowledgements ğŸ™

The concept of Agentic-RAG-R1 is inspired by [Deepseek-R1](https://arxiv.org/abs/2501.12948) and [TC-RAG](https://arxiv.org/abs/2408.09199). We sincerely appreciate the efforts of these teams for their contributions to open-source research and development.

## Citation ğŸ“

If you use this work in your research, please cite:

```bibtex
@misc{Agentic_RAG_R1,
  title       = {Agentic RAG-R1: Enhance Agentic RAG Reasoning Capacity via Reinforcement Learning},
  author      = {Xinke Jiang, Jiaran Gao, Rihong Qiu, Wentao Zhang, Yue Fang},
  year        = {2025},
  howpublished= {\url{https://github.com/jiangxinke/Agentic-RAG-R1}},
  note        = {GitHub repository},
}
```

## License ğŸ“„

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.
